> Intel-compatible processors also support an “extended-precision”
floating-point format with an 80-bit word divided into a sign bit, k=15 exponent
bits, a single integer bit, and n=63 fraction bits. The integer bit is an
explicit copy of the implied bit in the IEEE floating-point representation. That
is, it equals 1 for normalized values and 0 for denormalized values. Fill in the
following table giving the approximate values of some “interesting” numbers in
this format:
                                                Extended precision
Description                        Value                                Decimal
Smallest positive denormalized     2^(2-2^14) * 2^-63 = 2^(-2^14 - 63)  3.645E-4951
Smallest positive normalized                                2^(2-2^14)  3.362E-4932
Largest normalized                      2^(2^k - 2^(k-1) - 1) (2-2^-n)  1.190E4932

> This format can be used in C programs compiled for Intel-compatible machines
by declaring the data to be of type long double. However, it forces the compiler
to generate code based on the legacy 8087 floating-point instructions. The
resulting program will most likely run much slower than would be the case for
data type float or double.
