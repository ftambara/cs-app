> The imprecision of floating-point arithmetic can have disastrous effects. On
February 25, 1991, during the first Gulf War, an American Patriot Missile
battery in Dharan, Saudi Arabia, failed to intercept an incoming Iraqi Scud
missile. The Scud struck an American Army barracks and killed 28 soldiers. The
US General Accounting Office (GAO) conducted a detailed analysis of the failure
[76] and determined that the underlying cause was an imprecision in a numeric
calculation. In this exercise, you will reproduce part of the GAO’s analysis.
The Patriot system contains an internal clock, implemented as a counter that is
incremented every 0.1 seconds. To determine the time in seconds, the program
would multiply the value of this counter by a 24-bit quantity that was a
fractional binary approximation to 1/10. In particular, the binary
representation of 1/10 is the nonterminating sequence 0.000110011[0011]...2,
where the portion in brackets is repeated indefinitely. The program approximated
0.1, as a value x, by considering just the first 23 bits of the sequence to the
right of the binary point: x = 0.00011001100110011001100. (See Problem 2.51 for
a discussion of how they could have approximated 0.1 more precisely.)

> A. What is the binary representation of 0.1−x?

If we only keep 23 bits after the binary point, the error 0.1-x is:
```
                                23 bits
                                v
    x = 0.00011001100110011001100110011...
0.1-x = 0.00000000000000000000000[1100]...
```
Where the last four bits repeat periodically

> B. What is the approximate decimal value of 0.1 − x?

```
0001 1001 1001 1001 1001 100
0.1-x = .1 -
        (2^4 + 2^5 + 2^8 + 2^9 + 2^12 + 2^13 + 2^16 + 2^17 + 2^20 + 2^21) / 2^24
        = .1 - 1677720 / 16777216

0.1-x ~= 9.5367 10^-9
```
A simpler technique would be to realize that 0.1-x is 0.1 shifted right 20
positions. That means that `0.1-x = 0.1 / 2^20`

> C. The clock starts at 0 when the system is first powered up and keeps
counting up from there. In this case, the system had been running for around 100
hours. What was the difference between the actual time and the time computed by
the software?

At 100 hours the counter should mark 100*60*60*10=3600000 s.
```
actual_time - system_time = 3600000 s * (0.1-x) ~= 0.343 s
```

> D. The system predicts where an incoming missile will appear based on its
velocity and the time of the last radar detection. Given that a Scud travels at
around 2,000 meters per second, how far off was its prediction?

I think this question is very strangely worded. I think it does not give
enough information to know in which way the time difference affects the
calculations.
I assume the calculation that's meant to be done is
`2000 m/s * (1 - .343) s ~= 1313.35 m/s`, but it's almost whimsical.

> Normally, a slight error in the absolute time reported by a clock
reading would not affect a tracking computation. Instead, it should depend on
the relative time between two successive readings. The problem was that the
Patriot software had been upgraded to use a more accurate function for reading
time, but not all of the function calls had been replaced by the new code. As a
result, the tracking software used the accurate time for one reading and the
inaccurate time for the other [103].
